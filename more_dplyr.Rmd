```{r setup, echo=FALSE, warning=FALSE, purl=FALSE, message=FALSE}
options(repos="http://cran.rstudio.com/")
pkgs <- c("dplyr","knitr")
x<-lapply(pkgs, library, character.only = TRUE)
opts_chunk$set(tidy=T)
```

# More fun with `dplyr`
These materials will go over some introductory materials (i.e. from our [Intro R Workshop](https://github.com/rhodyrstats/intro_r_workshop)) as well as expand into some of the other functionality.  This presentation is heavily influenced by the [Introduction to `dplyr`](https://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html), the [Two-table Verbs](https://cran.r-project.org/web/packages/dplyr/vignettes/two-table.html), and [Databases](https://cran.r-project.org/web/packages/dplyr/vignettes/databases.html) vigenettes. In particular we will cover:

- [A word about pipes](#a-word-about-pipes)
- [Manipulating data](#manipulating-data)
- [Manipulating grouped data](#manipulating-grouped-data)
- [Database functionality: joins](#database-functionality-joins)
- [Database functionality: external databases](#database-functionality-external-databases)

## A word about pipes
There are many different ways to go about scripting an analyis in R (or any language for that matter).  These were discussed in the [Intro to R Workshop](https://github.com/rhodyrstats/intro_r_workshop/blob/master/lessons/03_wrangling.md#using-dplyr)(scroll down a bit), but to review, they are: using intermediate steps/objects, nest functions, or use pipes.  If you are developing new functions or packages it is probably best to not use pipes as it adds a dependency and de-bugging can be a bit of a challenge.  If you are scripting data analysis, pipes (i.e. `%>%` from `magrittr`) are, in my opinion, the way to go.  For this presentation we will use pipes for all the examples.

## Manipulating data
The `dplyr` package is first and foremost a package to help faciliatate data manipulation.  What it does can certainly be done with base R or with other packages, but it can be argued that `dplyr` makes these tasks more undertandable through its use of a consistent interface.  In particular, this is accomplished through the use of data manipulation verbs.  These verbs are:

- `select()`: selects columns from a data frame 
- `arrange()`: Arranges a data frame in ascending or descending order based on column(s). 
- `filter()`: Select observations from a data frame based on values in column(s).
- `slice()`: Selects observations based on specific rows 
- `rename()`: Rename columns in a dataframe
- `distinct()`: Get unique rows (OK, not a verb...) 
- `sample_n()`: Randomly select 'n' number of rows
- `sample_frac()`: Randomly select a fraction of rows 
- `mutate()`: Adds new columns to a data frame and keeps all other columns
- `transmutate()`: Adds new columns to a data frame and drops all other columns
- `summarise()`: Summarizes your data.

Before we move on, we need some data.  Once again, I am going to rely on the 2007 National Lakes Assessment Data:

```{r}
sites <- read.csv("http://www.epa.gov/sites/production/files/2014-01/nla2007_sampledlakeinformation_20091113.csv")
```

Let's look at the columns for each of these

```{r}
names(sites)
```

Given the large number of fields in these, I may want to reduce just to what I am interested in.

```{r}
sites_sel<-sites %>%
  select(SITE_ID,LAKENAME,VISIT_NO,SITE_TYPE,WSA_ECO9,AREA_HA,DEPTHMAX)
head(sites_sel)
```

We can arrange the data.

```{r}
#Ascending is default
sites_sel %>%
  arrange(DEPTHMAX) %>%
  head()
#Descending need desc()
sites_sel %>%
  arrange(desc(DEPTHMAX)) %>%
  head()
#By more than one column
sites_sel %>%
  arrange(WSA_ECO9,desc(DEPTHMAX))%>%
  head()
```

Let's filter out just some of the deeper lakes

```{r}
sites_sel %>%
  filter(DEPTHMAX >= 50)
```

Or just the deep lakes in Northern Applachians Ecoregion

```{r}
sites_sel %>%
  filter(WSA_ECO9 == "NAP", DEPTHMAX >= 50)
```

We can also grab observations by row

```{r}
sites_sel %>%
  slice(c(1,2))
#or
sites_sel %>%
  slice(seq(1,nrow(sites_sel),100))
```

Renaming columns is easy

```{r}
sites_sel %>%
  rename(Ecoregion = WSA_ECO9, MaxDepth = DEPTHMAX)%>%
  head()
```

We can identify distinct values and get those rows

```{r}
sites_sel %>%
  distinct(WSA_ECO9)
#Returns the first row with the distinct value so order has an impact
sites_sel %>%
  arrange(desc(DEPTHMAX))%>%
  distinct(WSA_ECO9)
```

Sampling by number or fraction and with or without replacment is done like:

```{r}
set.seed(72)
#By Number
sites_sel %>%
  sample_n(10)

#By Fraction
sites_sel %>%
  sample_frac(0.01)
```

To create new columns

```{r}
#Add it to the other columns
sites_sel %>%
  mutate(volume = ((10000*AREA_HA) * DEPTHMAX)/3)%>%
  head()
#Create only the new column
sites_sel %>%
  transmute(mean_depth = (((10000*AREA_HA) * DEPTHMAX)/3)/(AREA_HA*10000)) %>%
  head()
```

Lastly, we can get summaries of our data

```{r}
sites_sel %>%
  summarize(avg_depth = mean(DEPTHMAX,na.rm=T),
            n = n()) %>%
  head()
```

## Manipulating grouped data

- `group_by()` 
- `summarise()` 
- `n()` 
- `n_distinct()` 
- `first()` 
- `last()` 
- `nth()`

## Database functionality: joins
We will need to add another dataset to try some joins.  

```{r}
wq <- read.csv("http://www.epa.gov/sites/production/files/2014-10/nla2007_chemical_conditionestimates_20091123.csv")
wq_sel<-wq %>%
  select(SITE_ID,VISIT_NO,CHLA,NTL,PTL,TURB)
head(wq_sel)
```

- `left_join()` 
- `right_join()` 
- `inner_join()` 
- `full_join()`

Will let you explore the others on your own.
- `semi_join()` 
- `anti_join()` 
- `intersect()` 
- `union()` 
- `setdiff()`

## Database functionality: external databases

- `src_sqlite()` 
- `tbl()` 
- `collect()` 
- `translate_sql()`
- `copy_to()`

